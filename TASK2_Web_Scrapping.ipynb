{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtltRJ5cb392",
        "outputId": "22f1e9f2-8d0b-43c5-b264-0d7ed7bed1b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (2.25.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/lib/python3/dist-packages (4.10.0)\n",
            "Requirement already satisfied: pandas in /home/haseeb/.local/lib/python3.10/site-packages (2.1.1)\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.22.0-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m78.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm00:03\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /home/haseeb/.local/lib/python3.10/site-packages (from pandas) (1.26.0)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /home/haseeb/.local/lib/python3.10/site-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/haseeb/.local/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
            "Collecting trio~=0.17\n",
            "  Downloading trio-0.26.0-py3-none-any.whl (475 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.7/475.7 KB\u001b[0m \u001b[31m15.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/lib/python3/dist-packages (from selenium) (1.26.5)\n",
            "Collecting typing_extensions>=4.9.0\n",
            "  Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Collecting websocket-client>=1.8.0\n",
            "  Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 KB\u001b[0m \u001b[31m219.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Collecting certifi>=2021.10.8\n",
            "  Downloading certifi-2024.7.4-py3-none-any.whl (162 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.0/163.0 KB\u001b[0m \u001b[31m62.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: exceptiongroup in /home/haseeb/.local/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.1.3)\n",
            "Collecting attrs>=23.2.0\n",
            "  Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 KB\u001b[0m \u001b[31m151.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting sniffio>=1.3.0\n",
            "  Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: idna in /usr/lib/python3/dist-packages (from trio~=0.17->selenium) (3.3)\n",
            "Requirement already satisfied: sortedcontainers in /usr/lib/python3/dist-packages (from trio~=0.17->selenium) (2.1.0)\n",
            "Collecting outcome\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Collecting PySocks!=1.5.7,<2.0,>=1.5.6\n",
            "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m27.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: websocket-client, typing_extensions, sniffio, PySocks, h11, certifi, attrs, wsproto, outcome, trio, trio-websocket, selenium\n",
            "  Attempting uninstall: typing_extensions\n",
            "    Found existing installation: typing_extensions 4.8.0\n",
            "    Uninstalling typing_extensions-4.8.0:\n",
            "      Successfully uninstalled typing_extensions-4.8.0\n",
            "Successfully installed PySocks-1.7.1 attrs-23.2.0 certifi-2024.7.4 h11-0.14.0 outcome-1.3.0.post0 selenium-4.22.0 sniffio-1.3.1 trio-0.26.0 trio-websocket-0.11.1 typing_extensions-4.12.2 websocket-client-1.8.0 wsproto-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4 pandas selenium\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Web Scrapping using Beatiful Soap\n",
        "This Python script is designed to scrape data from a webpage, extract specific information from a table, and save the extracted data into a CSV file. The script leverages the requests library to fetch the webpage content, BeautifulSoup from the bs4 library to parse the HTML content, and pandas to handle the data and save it in a CSV format. Below is a detailed description of each part of the script.\n",
        "URL : https://www.scrapethissite.com/pages/forms/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Code Breakdown\n",
        "-Importing Libraries\n",
        "\n",
        "The script imports three essential libraries:\n",
        "\n",
        "requests: Used to make HTTP requests to fetch the webpage content.\n",
        "BeautifulSoup from bs4: Used to parse HTML and XML documents, facilitating the extraction of data from HTML tags.\n",
        "pandas: Used for data manipulation and analysis, particularly for creating and handling dataframes, which are then saved as CSV files.\n",
        "\n",
        "-Function to Extract Data\n",
        "\n",
        "extract_data(url): This function takes a URL as an input and attempts to scrape data from a table on the webpage.\n",
        "Sends an HTTP GET request to the specified URL.\n",
        "Checks if the request was successful (HTTP status code 200).\n",
        "Parses the webpage content using BeautifulSoup.\n",
        "Finds the table with the class 'table' on the webpage.\n",
        "If the table is found:\n",
        "Defines the column headers for the data.\n",
        "Initializes an empty list to store the extracted data.\n",
        "Finds all rows in the table.\n",
        "Iterates over each row in the table, skipping the header row.\n",
        "Finds all columns in the current row.\n",
        "Extracts and cleans text from each column.\n",
        "Appends the row data to the data list.\n",
        "Creates a pandas DataFrame from the data list with the specified headers.\n",
        "Returns the DataFrame.\n",
        "If the table is not found, prints a message and returns None.\n",
        "If the request fails, prints an error message with the status code and returns None.\n",
        "\n",
        "-Function to Save Data to CSV\n",
        "\n",
        "save_to_csv(df, filename='scrapethissite_data.csv'): This function takes a DataFrame and a filename as input and saves the DataFrame to a CSV file.\n",
        "Checks if the DataFrame is not None.\n",
        "Saves the DataFrame to a CSV file with the specified filename, without writing row indices.\n",
        "Prints a success message indicating the file has been saved.\n",
        "If the DataFrame is None, prints a message indicating that there is no data to save.\n",
        "\n",
        "-Main Function\n",
        "\n",
        "main(): This function orchestrates the data extraction and saving process.\n",
        "\n",
        "Specifies the URL of the webpage to scrape.\n",
        "Calls the extract_data function to scrape data from the specified URL.\n",
        "Calls the save_to_csv function to save the extracted data to a CSV file.\n",
        "if name == \"main\":: Checks if the script is being run as the main module.\n",
        "\n",
        "Calls the main function to execute the script.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSpBQSYdYAT-",
        "outputId": "a8ed600b-5e9e-4a1f-e74c-f3ee1864a919"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/haseeb/.local/lib/python3.10/site-packages/pandas/core/arrays/masked.py:62: UserWarning: Pandas requires version '1.3.4' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
            "  from pandas.core import (\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data successfully saved to 'scrapethissite_data.csv'\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def extract_data(url):\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        table = soup.find('table', class_='table')\n",
        "\n",
        "        if table:\n",
        "            headers = ['Team Name', 'Year', 'Wins', 'Losses', 'OT Losses', 'Win %', 'Goals For (GF)', 'Goals Against (GA)', '+ / -']\n",
        "            data = []\n",
        "\n",
        "            rows = table.find_all('tr')\n",
        "\n",
        "            for row in rows[1:]:  \n",
        "                cols = row.find_all('td')\n",
        "                row_data = [col.get_text(strip=True) for col in cols]\n",
        "                data.append(row_data)\n",
        "\n",
        "            df = pd.DataFrame(data, columns=headers)\n",
        "            return df\n",
        "        else:\n",
        "            print(\"Table not found on the page.\")\n",
        "            return None\n",
        "    else:\n",
        "        print(f\"Failed to retrieve data from {url}. Status code: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "def save_to_csv(df, filename='scrapethissite_data.csv'):\n",
        "    if df is not None:\n",
        "        df.to_csv(filename, index=False)\n",
        "        print(f\"Data successfully saved to '{filename}'\")\n",
        "    else:\n",
        "        print(\"No data to save.\")\n",
        "\n",
        "def main():\n",
        "    url = 'https://www.scrapethissite.com/pages/forms/'\n",
        "    data = extract_data(url)\n",
        "    save_to_csv(data)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
